{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcXw8yhGlMrG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u9YGAqTjBQS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importing citylearn lib and (modify) stablebaseline lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uqOKdRwLoo8H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/Forbu/CityLearn-1.3.5.git\n",
    "# !pip install git+https://github.com/Forbu/stable-baselines3.git\n",
    "# !git clone http://gitlab.aicrowd.com/adrien_forbu/neurips-2022-citylearn-challenge.git\n",
    "#\n",
    "# import os\n",
    "# path = \"/content/neurips-2022-citylearn-challenge\"\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1662575434935,
     "user": {
      "displayName": "Adrien Bufort",
      "userId": "14109265999645915407"
     },
     "user_tz": -120
    },
    "id": "tCqa7zEyWaji",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbwbDQrdjKNq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3011,
     "status": "ok",
     "timestamp": 1662622278392,
     "user": {
      "displayName": "Adrien Bufort",
      "userId": "14109265999645915407"
     },
     "user_tz": -120
    },
    "id": "Eh0MogD7lZxo",
    "outputId": "4a5b3dfe-d80a-47b5-d96b-56e8fc92b6fd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "C:\\Users\\philip\\Storage\\Epoch\\citylearn-2022-starter-kit\\venv\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "# import couple of libs some will be useful\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "# import stable_baselines3\n",
    "from stable_baselines3 import PPO, A2C, DDPG, TD3\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662622278392,
     "user": {
      "displayName": "Adrien Bufort",
      "userId": "14109265999645915407"
     },
     "user_tz": -120
    },
    "id": "QCe2JG3OoCDw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-NsWcBKjYlG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main tools\n",
    "\n",
    "Here we define the gym environment in a way that stable baseline 3 lib will be able to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1662622279250,
     "user": {
      "displayName": "Adrien Bufort",
      "userId": "14109265999645915407"
     },
     "user_tz": -120
    },
    "id": "OZNvHEKPllsh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = 'data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info.values())\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n",
    "\n",
    "import gym\n",
    "\n",
    "# here we init the citylearn env\n",
    "env = CityLearnEnv(schema=Constants.schema_path)\n",
    "\n",
    "#### IMPORTANT \n",
    "# here we choose the observation we want to take from the building env\n",
    "# we divide observation that are specific to buildings (index_particular)\n",
    "# and observation that are the same for all the buildings (index_commun)\n",
    "\n",
    "index_commun = [0, 2, 19, 4, 8, 24]\n",
    "index_particular = [20, 21, 22, 23]\n",
    "\n",
    "normalization_value_commun = [12, 24, 2, 100, 100, 1]\n",
    "normalization_value_particular = [5, 5, 5, 5]\n",
    "\n",
    "len_tot_index = len(index_commun) + len(index_particular) * 5\n",
    "\n",
    "## env wrapper for stable baselines\n",
    "class EnvCityGym(gym.Env):\n",
    "    \"\"\"\n",
    "    Env wrapper coming from the gym library.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "        # get the number of buildings\n",
    "        self.num_buildings = len(env.action_space)\n",
    "\n",
    "        # define action and observation space\n",
    "        self.action_space = gym.spaces.Box(low=np.array([-1] * self.num_buildings), high=np.array([1] * self.num_buildings), dtype=np.float32)\n",
    "\n",
    "        # define the observation space\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0] * len_tot_index), high=np.array([1] * len_tot_index), dtype=np.float32)\n",
    "\n",
    "        # TO THINK : normalize the observation space\n",
    "\n",
    "    def reset(self):\n",
    "        obs_dict = env_reset(self.env)\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        observation = self.get_observation(obs)\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def get_observation(self, obs):\n",
    "        \"\"\"\n",
    "        We retrieve new observation from the building observation to get a proper array of observation\n",
    "        Basicly the observation array will be something like obs[0][index_commun] + obs[i][index_particular] for i in range(5)\n",
    "\n",
    "        The first element of the new observation will be \"commun observation\" among all building like month / hour / carbon intensity / outdoor_dry_bulb_temperature_predicted_6h ...\n",
    "        The next element of the new observation will be the concatenation of certain observation specific to buildings non_shiftable_load / solar_generation / ...  \n",
    "        \"\"\"\n",
    "        \n",
    "        # we get the observation commun for each building (index_commun)\n",
    "        observation_commun = [obs[0][i]/n for i, n in zip(index_commun, normalization_value_commun)]\n",
    "        observation_particular = [[o[i]/n for i, n in zip(index_particular, normalization_value_particular)] for o in obs]\n",
    "\n",
    "        observation_particular = list(itertools.chain(*observation_particular))\n",
    "        # we concatenate the observation\n",
    "        observation = observation_commun + observation_particular\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        we apply the same action for all the buildings\n",
    "        \"\"\"\n",
    "        # reprocessing action\n",
    "        action = [[act] for act in action]\n",
    "\n",
    "        # we do a step in the environment\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "        observation = self.get_observation(obs)\n",
    "\n",
    "        return observation, sum(reward), done, info\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCzGbZmSlYg0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train and test function\n",
    "\n",
    "The function to train and test the sb3 PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SrJsJWxjjj4g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# function to train the policy with PPO algorithm\n",
    "def test_ppo():\n",
    "\n",
    "    # Modify the petting zoo environment to make a custom observation space (return an array of value for each agent)\n",
    "    \n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    env = EnvCityGym(env)\n",
    "    \n",
    "    # we load the model\n",
    "    model = PPO.load(\"ppo_citylearn\")\n",
    "\n",
    "    # we reset the environment\n",
    "    obs = env.reset()\n",
    "\n",
    "    nb_iter = 8000\n",
    "\n",
    "    # loop on the number of iteration\n",
    "    for i in range(nb_iter):\n",
    "        # we get the action for each agent\n",
    "        actions = []\n",
    "        for agent in env.possible_agents:\n",
    "            action, _states = model.predict(obs[agent], deterministic=True)\n",
    "\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        actions = {agent: action for agent, action in zip(env.possible_agents, actions)}\n",
    "\n",
    "        # we do a step in the environment\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "\n",
    "        # sometimes check the actions and rewards\n",
    "        if i % 100 == 0:\n",
    "            print(\"actions : \", actions)\n",
    "            print(\"rewards : \", rewards)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    final_result = sum(env.citylearnenv.evaluate())/2\n",
    "\n",
    "    print(\"final result : \", final_result)\n",
    "    # launch as main\n",
    "\n",
    "    return final_result\n",
    "    \n",
    "\n",
    "# function to train the policy with PPO algorithm\n",
    "def train_ppo():\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    env = EnvCityGym(env)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the algorithm\n",
    "\n",
    "    # load model if exist\n",
    "    try:\n",
    "        model = PPO.load(\"ppo_citylearn\")\n",
    "    except:\n",
    "        model = PPO('MlpPolicy', env, verbose=2, gamma=0.99)\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=2000)\n",
    "\n",
    "    model.save(\"ppo_citylearn\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29120254,
     "status": "ok",
     "timestamp": 1662651399502,
     "user": {
      "displayName": "Adrien Bufort",
      "userId": "14109265999645915407"
     },
     "user_tz": -120
    },
    "id": "U6DiqRA1lpsH",
    "outputId": "2e699b43-ca8e-47be-b30d-989f385c537a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 639  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x1b2be7cba30>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ppo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16577,
     "status": "ok",
     "timestamp": 1662651416071,
     "user": {
      "displayName": "Adrien Bufort",
      "userId": "14109265999645915407"
     },
     "user_tz": -120
    },
    "id": "x6wWwpr0noj4",
    "outputId": "427b78d0-4a54-4d99-9e05-4074863a2fac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philip\\Storage\\Epoch\\citylearn-2022-starter-kit\\venv\\lib\\site-packages\\gym\\spaces\\box.py:112: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions :  [-1.        -0.9417206 -1.        -0.6322116 -1.       ]\n",
      "rewards :  -1.7605267429862221\n",
      "actions :  [-1.         -0.06260055 -0.89389765 -0.80249417  0.5648109 ]\n",
      "rewards :  -2.012850283562906\n",
      "actions :  [-0.18859935 -1.         -1.          1.         -1.        ]\n",
      "rewards :  0.0\n",
      "actions :  [-1.         -0.9617046  -1.          0.34298924 -1.        ]\n",
      "rewards :  -0.8239999768115437\n",
      "actions :  [ 0.20784858 -0.18209967 -1.         -0.5072852  -0.04612648]\n",
      "rewards :  -2.421145669993607\n",
      "actions :  [-1.         -0.43406224  0.19246817  0.00745602 -0.05590498]\n",
      "rewards :  -1.186464684597307\n",
      "actions :  [-1.         -1.         -1.         -0.8782959  -0.66875494]\n",
      "rewards :  -0.4946580346444852\n",
      "actions :  [-0.5432862  -1.          0.09719436  0.47274664  0.42673296]\n",
      "rewards :  -1.230325556764777\n",
      "actions :  [-0.4650563 -1.        -1.         1.        -0.8139769]\n",
      "rewards :  -0.5466462325970047\n",
      "1.0349857910443503\n",
      "-14968.379574946124\n"
     ]
    }
   ],
   "source": [
    "def evaluate_print_results(model, print_substeps=False):\n",
    "    # simple run though the env with our PPO policy and we sometimes print our actions / reward to get a sense of what we are doing\n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    env = EnvCityGym(env)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    # model = PPO.load(\"ppo_citylearn\")\n",
    "\n",
    "    nb_iter = 8750\n",
    "\n",
    "    reward_tot = 0\n",
    "\n",
    "    for i in range(nb_iter):\n",
    "\n",
    "        action = model.predict(obs)[0]\n",
    "\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        reward_tot += rewards\n",
    "\n",
    "        if print_substeps:\n",
    "            if i % 1000 == 0:\n",
    "                print(\"actions : \", action)\n",
    "                print(\"rewards : \", rewards)\n",
    "\n",
    "    print(sum(env.env.evaluate())/2)\n",
    "    print(reward_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function to train the policy with PPO algorithm\n",
    "def train_ddpg():\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    env = EnvCityGym(env)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the algorithm\n",
    "\n",
    "    # load model if exist\n",
    "    try:\n",
    "        model = DDPG.load(\"ddpg_citylearn\")\n",
    "    except:\n",
    "        model = DDPG('MlpPolicy', env, verbose=2, gamma=0.99)\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=200000)\n",
    "\n",
    "    model.save(\"ddpg_citylearn\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 8.76e+03  |\n",
      "|    ep_rew_mean     | -1.46e+04 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 54        |\n",
      "|    time_elapsed    | 646       |\n",
      "|    total_timesteps | 35036     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.95e+03 |\n",
      "|    critic_loss     | 860       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 26277     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# function to train the policy with PPO algorithm\n",
    "def train_a2c():\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    env = EnvCityGym(env)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the algorithm\n",
    "\n",
    "    # load model if exist\n",
    "    try:\n",
    "        model = A2C.load(\"a2c_citylearn\")\n",
    "    except:\n",
    "        model = A2C('MlpPolicy', env, verbose=2, gamma=0.99)\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=200000)\n",
    "\n",
    "    model.save(\"a2c_citylearn\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_a2c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function to train the policy with PPO algorithm\n",
    "def train_td3():\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    env = EnvCityGym(env)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the algorithm\n",
    "\n",
    "    # load model if exist\n",
    "    try:\n",
    "        model = TD3.load(\"td3_citylearn\")\n",
    "    except:\n",
    "        model = TD3('MlpPolicy', env, verbose=2, gamma=0.99)\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=200000)\n",
    "\n",
    "    model.save(\"td3_citylearn\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_td3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"######PPO#######\")\n",
    "\n",
    "model = PPO.load(\"ppo_citylearn\")\n",
    "evaluate_print_results(model)\n",
    "\n",
    "print(\"######DDPG#######\")\n",
    "\n",
    "model = DDPG.load(\"ddpg_citylearn\")\n",
    "evaluate_print_results(model)\n",
    "\n",
    "print(\"######A2C#######\")\n",
    "\n",
    "model = DDPG.load(\"a2c_citylearn\")\n",
    "evaluate_print_results(model)\n",
    "\n",
    "print(\"######TD3#######\")\n",
    "\n",
    "model = DDPG.load(\"td3_citylearn\")\n",
    "evaluate_print_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFonvZmxllig",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GUYS WE ARE AT 0.95 ! BELOW 1.0 !"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUyE5R9iPjI4eCvmNTkQZK",
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1nVaSlhoXP6-csOuDxU6IE37BdPca_x3F",
     "timestamp": 1662403569200
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}