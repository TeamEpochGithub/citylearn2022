{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import FloatTensor,Tensor,LongTensor\n",
    "from deterministicpolicy import DeterministicActorCritic # REINFORCE, ActorCritic,\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from traineval.training.custom_DRL.drlmain.spinning_up_evaluation import evaluate\n",
    "import joblib\n",
    "import timeit\n",
    "from traineval.utils.register_environment import register_environment\n",
    "from traineval.utils.convert_arguments import get_environment_arguments\n",
    "from traineval.training.spinningup.environments import epoch_citylearn\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "\n",
    "def AC(total_episodes, estimation_depth, learning_rate, gradient_method, hidden_shape_actor, hidden_shape_critic, epsilon):\n",
    "    \"\"\"\n",
    "    Tries to solve Cartpole-v1 usinf the REINFORCE algorithm. Right now it only applies a Monte-Carlo REINFORCE\n",
    "\n",
    "    Args:\n",
    "        total_episodes: How many times the environment resets\n",
    "        learning_rate: For optimizer\n",
    "        future_reward_discount_factor: future rewards are dicounted\n",
    "        hidden_shape: List of integers. [16,16] would give two hidden layers (linear with PReLU activation) with both 16 nodes in the policy model\n",
    "\n",
    "    Returns:\n",
    "        scores: Score per episode in a list\n",
    "    \"\"\"\n",
    "\n",
    "    district_args = [\"hour\",\n",
    "                     \"month\",\n",
    "                     \"carbon_intensity\",\n",
    "                     \"electricity_pricing\"]\n",
    "\n",
    "    building_args = [\"non_shiftable_load\",\n",
    "                     \"solar_generation\",\n",
    "                     \"electrical_storage_soc\",\n",
    "                     \"net_electricity_consumption\"]\n",
    "    #\n",
    "    #\n",
    "    # district_args = []\n",
    "    # building_args = [\"electrical_storage_soc\"]\n",
    "\n",
    "    environment_arguments = get_environment_arguments(district_args, building_args)\n",
    "    register_environment(environment_arguments)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    env = gym.make(\"Epoch-Citylearn-v1\", disable_env_checker=True)\n",
    "\n",
    "    action_space = 1\n",
    "    gamma = 0.95\n",
    "    agent = DeterministicActorCritic(env.observation_space.shape[0], action_space, estimation_depth, gamma, gradient_method, learning_rate, hidden_shape_actor, hidden_shape_critic, epsilon)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(total_episodes):\n",
    "        #reset the environment\n",
    "        obs = env.reset()\n",
    "\n",
    "        t = 0\n",
    "\n",
    "        while t < 8760 - 1:\n",
    "            daily_averages = []\n",
    "\n",
    "            observations = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            next_observations = []\n",
    "            dones = []\n",
    "\n",
    "            for x in range(24):\n",
    "\n",
    "                # print(t, x)\n",
    "                t += 1\n",
    "\n",
    "                observations.append(obs)\n",
    "\n",
    "                #Action selection is done by the policy\n",
    "                action = agent.pick(obs)\n",
    "                actions.append(action)\n",
    "\n",
    "                #Get example\n",
    "                obs, reward, done, _ = env.step(action.tolist()[0] * 5)\n",
    "                # reward = (((reward - (-3)) * (1 - (-0))) / (-0.2 - (-3))) + (-0)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                next_observations.append(obs)\n",
    "                dones.append(done)\n",
    "\n",
    "                if done or x == 24 - 1:\n",
    "                    daily_average = np.mean(rewards)\n",
    "                    daily_averages.append(daily_average)\n",
    "                    # print(f\"Y {i} D {t/24} finished, mean score: {daily_average.round(2)}, last action: {actions[-1][0][0]}\")\n",
    "                    scores.append(np.mean(rewards))\n",
    "                    break\n",
    "\n",
    "            observations = torch.FloatTensor(np.array(observations))\n",
    "            actions = torch.cat(actions)\n",
    "            next_observations = torch.FloatTensor(np.array(next_observations))\n",
    "            dones = torch.Tensor(np.array(dones))\n",
    "\n",
    "            rewards = torch.FloatTensor(np.full((1, len(rewards)), np.mean(rewards))[0])\n",
    "\n",
    "            agent.update(data={\"obs\":observations,\"act\":actions,\"rew\":rewards,\"obs2\":next_observations,\"done\":dones})\n",
    "\n",
    "            torch.save(agent.retrieve_actor(), \"wowamodela.pt\")\n",
    "\n",
    "            if t == 8759:\n",
    "                print(f\"Y {i} finished, mean score: {np.mean(daily_averages).round(2)}\")\n",
    "\n",
    "        # Following doesn't work, probably keeps trying the same model\n",
    "        # if i % 5000 == 0:\n",
    "        #     evaluate(environment_arguments, \"ppo\", 0, 0, True)\n",
    "\n",
    "        # year_mean = np.average(rewards)\n",
    "        # scores.append(year_mean)\n",
    "        # print(f\"Year {i} done with average reward {year_mean}\")\n",
    "\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y 0 finished, mean score: -1.28\n",
      "Y 1 finished, mean score: -1.0\n",
      "Y 2 finished, mean score: -1.27\n",
      "Y 3 finished, mean score: -0.96\n",
      "Y 4 finished, mean score: -0.69\n",
      "Y 5 finished, mean score: -0.67\n",
      "Y 6 finished, mean score: -0.74\n",
      "Y 7 finished, mean score: -1.39\n",
      "Y 8 finished, mean score: -0.67\n",
      "Y 9 finished, mean score: -0.66\n",
      "Y 10 finished, mean score: -1.22\n",
      "Y 11 finished, mean score: -0.5\n",
      "Y 12 finished, mean score: -0.45\n",
      "Y 13 finished, mean score: -0.93\n",
      "Y 14 finished, mean score: -0.82\n",
      "Y 15 finished, mean score: -1.6\n",
      "Y 16 finished, mean score: -1.61\n",
      "Y 17 finished, mean score: -1.93\n",
      "Y 18 finished, mean score: -1.6\n",
      "Y 19 finished, mean score: -1.86\n",
      "Y 20 finished, mean score: -1.8\n",
      "Y 21 finished, mean score: -1.85\n",
      "Y 22 finished, mean score: -1.69\n",
      "Y 23 finished, mean score: -1.66\n",
      "Y 24 finished, mean score: -1.77\n",
      "Y 25 finished, mean score: -1.9\n",
      "Y 26 finished, mean score: -1.66\n",
      "Y 27 finished, mean score: -1.92\n",
      "Y 28 finished, mean score: -1.79\n",
      "Y 29 finished, mean score: -1.89\n",
      "Y 30 finished, mean score: -2.0\n",
      "Y 31 finished, mean score: -1.93\n",
      "Y 32 finished, mean score: -1.75\n",
      "Y 33 finished, mean score: -1.83\n",
      "Y 34 finished, mean score: -1.62\n",
      "Y 35 finished, mean score: -1.7\n",
      "Y 36 finished, mean score: -1.97\n",
      "Y 37 finished, mean score: -2.0\n",
      "Y 38 finished, mean score: -1.91\n",
      "Y 39 finished, mean score: -1.55\n",
      "Y 40 finished, mean score: -1.93\n",
      "Y 41 finished, mean score: -1.94\n",
      "Y 42 finished, mean score: -1.91\n",
      "Y 43 finished, mean score: -1.6\n",
      "Y 44 finished, mean score: -1.69\n",
      "Y 45 finished, mean score: -1.63\n",
      "Y 46 finished, mean score: -1.75\n",
      "Y 47 finished, mean score: -1.75\n",
      "Y 48 finished, mean score: -1.8\n",
      "Y 49 finished, mean score: -1.59\n",
      "Y 50 finished, mean score: -1.94\n",
      "Y 51 finished, mean score: -1.93\n",
      "Y 52 finished, mean score: -1.87\n",
      "Y 53 finished, mean score: -1.92\n",
      "Y 54 finished, mean score: -1.87\n",
      "Y 55 finished, mean score: -2.01\n",
      "Y 56 finished, mean score: -1.81\n",
      "Y 57 finished, mean score: -1.92\n",
      "Y 58 finished, mean score: -1.92\n",
      "Y 59 finished, mean score: -1.7\n",
      "Y 60 finished, mean score: -1.93\n",
      "Y 61 finished, mean score: -1.76\n",
      "Y 62 finished, mean score: -1.92\n",
      "Y 63 finished, mean score: -1.75\n",
      "Y 64 finished, mean score: -1.71\n",
      "Y 65 finished, mean score: -1.89\n",
      "Y 66 finished, mean score: -1.86\n",
      "Y 67 finished, mean score: -1.83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [38], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m hidden_shape_critic \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m64\u001B[39m]\n\u001B[0;32m      8\u001B[0m epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.3\u001B[39m\n\u001B[1;32m---> 10\u001B[0m scores_to_plot \u001B[38;5;241m=\u001B[39m AC(total_episodes, estimation_depth, learning_rate, gradient_method, hidden_shape_actor, hidden_shape_critic, epsilon)\n",
      "Cell \u001B[1;32mIn [37], line 75\u001B[0m, in \u001B[0;36mAC\u001B[1;34m(total_episodes, estimation_depth, learning_rate, gradient_method, hidden_shape_actor, hidden_shape_critic, epsilon)\u001B[0m\n\u001B[0;32m     72\u001B[0m observations\u001B[38;5;241m.\u001B[39mappend(obs)\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m#Action selection is done by the policy\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpick\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m actions\u001B[38;5;241m.\u001B[39mappend(action)\n\u001B[0;32m     78\u001B[0m \u001B[38;5;66;03m#Get example\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Epoch III\\citylearn-2022-starter-kit\\traineval\\training\\custom_DRL\\drlmain\\deterministicpolicy.py:125\u001B[0m, in \u001B[0;36mDeterministicActorCritic.pick\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    123\u001B[0m p \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandom()\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon:\n\u001B[1;32m--> 125\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m (\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_possible_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m)\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    127\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpick_greedy(state)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "total_episodes = 300\n",
    "learning_rate = 0.001\n",
    "estimation_depth = 500  ## DEPR\n",
    "gradient_method = 'both' ## DEPR\n",
    "hidden_shape_actor = [64, 64]\n",
    "hidden_shape_critic = [64, 64]\n",
    "epsilon = 0.3\n",
    "\n",
    "scores_to_plot = AC(total_episodes, estimation_depth, learning_rate, gradient_method, hidden_shape_actor, hidden_shape_critic, epsilon)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(scores_to_plot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "days = np.mean(np.array([scores_to_plot[i*24:i*24+24]]) for i in range(100*365), axis = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "years = np.mean(np.array([scores_to_plot[i*365:i*365+365] for i in range(10)]), axis = 0)\n",
    "plt.plot(years)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
