{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BABY STEPS - Getting Started\n",
    "\n",
    "```\n",
    "Author: Chia E Tungom\n",
    "Email: bamtungom@protonmail.com\n",
    "```\n",
    "\n",
    "This Notebook demonstrates the basic facets of the CityLearn Environment. You can play with it to get familiar with the environment.\n",
    "Important aspects of the environment that covered include include:\n",
    "\n",
    "1. Observation Space (dataset)\n",
    "\n",
    "2. Action Space (discrete or continous)\n",
    "\n",
    "3. Model (Policy)\n",
    "\n",
    "4. Action (steps)\n",
    "\n",
    "5. Evaluation (reward)\n",
    "\n",
    "We use general purpose functions common to most RL environments for illustration.\n",
    "\n",
    "__Note:__ To run this notebook, place it in the root directory of your CityLearn Phase one repository (same directory as requirements.txt)\n",
    "\n",
    "__Lets Goooooo!!!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Please do not make changes to this file. \n",
    "This is only a reference script provided to allow you \n",
    "to do local evaluation. The evaluator **DOES NOT** \n",
    "use this script for orchestrating the evaluations. \n",
    "\"\"\"\n",
    "\n",
    "# to avoid crashes but might cause results to be different \n",
    "# https://github.com/dmlc/xgboost/issues/1715\n",
    "\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "from agents.orderenforcingwrapper import OrderEnforcingAgent\n",
    "from citylearn.citylearn import CityLearnEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Custom configure enviroment \n",
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info.values())\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Define Environment\n",
    "\n",
    "The first thing we need to do is create a CityLearn environment. The environment is defined using a json schema and dataset which can be found in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Understand CityLearn Environment\n",
    "\n",
    "env = CityLearnEnv(schema=Constants.schema_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. OBSERVATION SPACE\n",
    "\n",
    "The observation space is the data of the environment. This is what the agent sees inorder to decide which action to take.\n",
    "\n",
    "Based on our environment the observation space is 5 dimensional corresponding to the number of buildings. Each building has it's own observation which is a 28 dimension 1D array. The 1D array stands for an observation at one point in time. Therefore our environment is a `5x28` array\n",
    "\n",
    "1. Use `env.observation_space` to explore the entire environment\n",
    "2. Use `env.observation_space[index]` to esplore the envrionment of a particular building (index 0 for building 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SAMPLE OBSERVATION SPACE for Builiding ONE >>> (28, array([ 2.2306538e+00,  4.6289158e+00,  1.9862038e+01,  1.8659870e+01,\n",
      "        2.5638412e+01,  3.1128130e+01,  1.2003634e+01,  7.9094368e+01,\n",
      "        3.0085012e+01,  5.3143150e+01,  5.9671593e+01,  5.3548183e+01,\n",
      "        1.1706564e+02,  1.0135307e+03,  8.1144623e+02,  4.4872473e+02,\n",
      "        6.3407776e+02,  4.0278061e+01,  8.6934454e+02, -7.5822127e-01,\n",
      "        1.1431421e+00,  6.4957153e+02,  1.5340083e+00, -8.5630109e+02,\n",
      "       -6.6790152e-01, -4.0250856e-01,  7.3932028e-01,  8.4803706e-01],\n",
      "      dtype=float32))\n",
      " SAMPLE OBSERVATION SPACE for Builiding ONE >>> (28, array([ 8.43912792e+00,  1.77980804e+00,  1.36043034e+01,  2.01663532e+01,\n",
      "        1.48673620e+01,  2.83870754e+01,  3.12863426e+01,  7.55391541e+01,\n",
      "        3.86182785e+01,  6.17445641e+01,  3.65973358e+01,  1.28817825e+02,\n",
      "        1.38044373e+02,  8.38174438e+02,  4.46565933e+01,  7.66775146e+02,\n",
      "        5.84005188e+02,  6.84828369e+02,  6.10748108e+02,  3.43578786e-01,\n",
      "        6.49972010e+00,  4.08617828e+02,  1.24374330e+00, -2.11832333e+00,\n",
      "        8.72274876e-01,  1.02183664e+00, -1.00081496e-01, -6.12758219e-01],\n",
      "      dtype=float32))\n",
      " SAMPLE OBSERVATION SPACE for Builiding ONE >>> (28, array([ 1.0540931e+01,  7.5837021e+00,  1.6375147e+01,  1.8926344e+01,\n",
      "        1.9624830e+01,  1.5956203e+01,  8.4171324e+00,  5.4042168e+01,\n",
      "        3.4934280e+01,  8.3116669e+01,  7.3257561e+01,  1.8487917e+01,\n",
      "        7.8556702e+01,  9.1675409e+02,  3.1244525e+02,  3.4421002e+02,\n",
      "        7.2111859e+02,  1.4277869e+02,  4.4374307e+02,  7.3715448e-01,\n",
      "        3.2433646e+00,  3.2299454e+02, -9.4254714e-01, -7.8422760e+02,\n",
      "        9.2990977e-01,  6.7099714e-01, -1.7191057e-01,  1.0709059e+00],\n",
      "      dtype=float32))\n",
      " SAMPLE OBSERVATION SPACE for Builiding ONE >>> (28, array([ 3.9380374e+00,  3.7980776e+00,  3.3955061e+00,  3.1017109e+01,\n",
      "        1.1978448e+01,  5.3095875e+00,  1.6591652e+01,  3.9511307e+01,\n",
      "        9.3647966e+00,  9.1305435e+01,  9.7392149e+00,  9.3440027e+02,\n",
      "        5.6966589e+02,  4.3525198e+02,  5.0421936e+01,  1.8403453e+02,\n",
      "        6.4827277e+02,  4.9743362e+02,  8.0842126e+02, -6.1989862e-01,\n",
      "        2.8023574e+00,  6.5361761e+02,  1.5519366e+00,  2.0607713e+02,\n",
      "       -4.2610571e-01,  1.8368478e-01,  1.3570160e+00,  1.0172350e+00],\n",
      "      dtype=float32))\n",
      " SAMPLE OBSERVATION SPACE for Builiding ONE >>> (28, array([ 1.1954871e+01,  4.3262911e+00,  1.0124896e+01,  2.1151545e+01,\n",
      "        1.5598174e+01,  2.9132732e+01,  1.2610396e+01,  1.5448365e+01,\n",
      "        6.7179947e+01,  5.7224922e+01,  2.5089495e+01,  5.1380375e+01,\n",
      "        4.5354843e+01,  9.5390771e+02,  3.8694989e+02,  3.5592596e+02,\n",
      "        1.7225145e+02,  5.7204333e+02,  4.2664621e+02,  1.2165537e+00,\n",
      "        1.6440982e+00,  7.4219165e+02, -3.3068177e-01,  4.8726962e+02,\n",
      "        8.0581689e-01, -1.0057686e-01, -1.4271154e-01,  1.1210358e+00],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# There is an action space for every building\n",
    "# print(f' OBSERVATION SPACES {env.observation_space}')\n",
    "# print(f' OBSERVATION SPACE for Builiding ONE is {env.observation_space[0]}')\n",
    "\n",
    "# sample some actions\n",
    "for building in range(5):\n",
    "    print(f' SAMPLE OBSERVATION SPACE for Builiding ONE >>> {len(env.observation_space[building].sample()), env.observation_space[building].sample()}')\n",
    "\n",
    "# we can see the observations are a 28 1D numpy array with every dimension defined by the range given in the spaces BOX "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. ACTION SPACE\n",
    "\n",
    "This shows us the type of actions we can take along with the dimension and property (discrete of contineous) of each actions. In the citylearn challenge, the actions are continous and one dimensional in the range [-1,1] for each building. 1 means charging and -1 means discharging.\n",
    "\n",
    "- Based on our environment, the action space is a 5 dimensional array with each array corresponding to the action space of a building.\n",
    "- one array is of the form `[(-1,1), (1,), float32]` which correspond to `[(lower bound, upper bound), (dimension,), datatype]`\n",
    "- __lower bound__ is the lowest or smallest value of an action while __upper bound__ is the highest.\n",
    "- Dimension stands for  of our action which here is 1 (use `action_space.sample()` to see an action)\n",
    "- Datatype is the data type of our action which here is float\n",
    "\n",
    "The cell below illustrates the action space(s). Play with it for understanding the actions.\n",
    "\n",
    "`action_space.sample` produces a random actions\n",
    "\n",
    "Note: You must pick an action space of a given building inorder to sample (use index e.g `action_space[0]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACTION SPACES [Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32), Box(-1.0, 1.0, (1,), float32)]\n",
      " ACTION SPACE for Builiding ONE is Box(-1.0, 1.0, (1,), float32)\n",
      " SAMPLE ACTION SPACE for Builiding ONE >>> [-0.18105811]\n",
      " SAMPLE ACTION SPACE for Builiding ONE >>> [0.02933011]\n",
      " SAMPLE ACTION SPACE for Builiding ONE >>> [0.5736601]\n",
      " SAMPLE ACTION SPACE for Builiding ONE >>> [0.59951377]\n",
      " SAMPLE ACTION SPACE for Builiding ONE >>> [0.2557987]\n"
     ]
    }
   ],
   "source": [
    "# There is an action space for every building\n",
    "print(f' ACTION SPACES {env.action_space}')\n",
    "print(f' ACTION SPACE for Builiding ONE is {env.action_space[0]}')\n",
    "\n",
    "# sample some actions\n",
    "for action in range(5):\n",
    "    print(f' SAMPLE ACTION SPACE for Builiding ONE >>> {env.action_space[1].sample()}')\n",
    "\n",
    "# we can observe the actions are continous in the range [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Define A Model or Agent \n",
    "\n",
    "The agent is the Policy which decides what action to take given an observation. We can use Rule based actions(agents). The CityLearn setting is built for multiagent systems but a single agent can aslo be used.\n",
    "\n",
    "Here we just show how to load an agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from citylearn.agents.sac import SAC\n",
    "\n",
    "# SAC??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. TAKING AN ACTION\n",
    "\n",
    "As already explained with the action spaces, $n$ buildings will have $n$ actions with each action corresponding to one building. Therefore our actions should appear as follows\n",
    "\n",
    "- Action should be a List containing tuples(number of buildings). inside the tuple is a list conatining the action corresponding to the action to be taken for a given building\n",
    "- Example for a five buildings environment, we could have.\n",
    "\n",
    "``` python\n",
    "\n",
    "Actions = [ ([0.0]), ([0.0]), ([0.0]), ([0.0]), ([0.0]) ]\n",
    "\n",
    "```\n",
    "A list of list is also acceptable\n",
    "\n",
    "``` python\n",
    "\n",
    "Actions = [ [0.0], [0.0], [0.0], [0.0], [0.0] ]\n",
    "\n",
    "```\n",
    "\n",
    "We take an action when we want to move one step ahead. We can do this using `env.step(action)`\n",
    "\n",
    "When we take an action the output contains a tuple with the following:\n",
    "\n",
    "1. Next State\n",
    "2. Reward\n",
    "3. If the state is a Terminal State\n",
    "4. Information about the environment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WE are about to take [[0.2562346777804527], [-0.06318111831231477], [0.8458174785046355], [-0.5422101607408161], [0.25202092295158907]] \n",
      "\n",
      " NEXT STATE \n",
      " [[8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 0.8511666666666671, 0.0, 0.23412847854639585, 2.4910686044615646, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 1.3706666666666665, 0.0, 0.0, 1.3706666666666665, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 1.0185241699218762e-07, 0.0, 0.7202769107260067, 5.000000101852417, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 1.9281666666666664, 0.0, 0.0, 1.9281666666666664, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 0.5158833333333334, 0.0, 0.23014805225059506, 2.1288172402235035, 0.22, 0.22, 0.22, 0.22]] \n",
      "\n",
      " REWARDS [-0.93992766 -0.51717866 -1.88659533 -0.72753403 -0.80324332] \n",
      "\n",
      " TERMINAL OR NOT >> False \n",
      "\n",
      " INFO {}\n"
     ]
    }
   ],
   "source": [
    "# print(env_reset(env)[\"action_space\"])\n",
    "# env_reset(env)[\"observation_space\"]\n",
    "# env.reset()[0]\n",
    "\n",
    "import random\n",
    "Actions = [([random.uniform(-1,1)]) for _ in range(5)]\n",
    "print(f' WE are about to take {Actions} \\n')\n",
    "next_state, reward, terminal, info = env.step(Actions)\n",
    "\n",
    "print(f' NEXT STATE \\n {next_state} \\n')\n",
    "print(f' REWARDS {reward} \\n')\n",
    "print(f' TERMINAL OR NOT >> {terminal} \\n')\n",
    "print(f' INFO {info}')\n",
    "\n",
    "\n",
    "# obs_dict = env_reset(env)\n",
    "# agent = OrderEnforcingAgent()\n",
    "# print(agent.register_reset(obs_dict))\n",
    "# env.step(agent.register_reset(obs_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Evaluating Actions\n",
    "\n",
    "After Taking actions we can evaluate the performance of our agent or agents.\n",
    "\n",
    "Evalution is done using the final metric which is the price cost and Emission cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1.648755986204545, 1.6155431197943488)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SAMPLE RUN or LOCAL EVALUATION\n",
    "\n",
    "Some modification have been made from the origial code. For isinstance\n",
    "\n",
    "- We can run a test for a month i.e $30*24$ to quickly evaluate our agent \n",
    "\n",
    "we add the following code in the evaluation section \n",
    "\n",
    "``` python \n",
    "\n",
    "    # Skipping to shorten training time\n",
    "    days = 30*5\n",
    "    training_steps = 24*days\n",
    "    skipping = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local evaluation\n",
      "Num Steps: 1000, Num episodes: 0\n",
      "Num Steps: 2000, Num episodes: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Please do not make changes to this file. \n",
    "This is only a reference script provided to allow you \n",
    "to do local evaluation. The evaluator **DOES NOT** \n",
    "use this script for orchestrating the evaluations. \n",
    "\"\"\"\n",
    "\n",
    "from agents.orderenforcingwrapper import OrderEnforcingAgent\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "class Constants:\n",
    "    episodes = 5\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info.values())\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    print(\"Starting local evaluation\")\n",
    "    \n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    agent = OrderEnforcingAgent()\n",
    "\n",
    "    obs_dict = env_reset(env)\n",
    "\n",
    "    agent_time_elapsed = 0\n",
    "\n",
    "    step_start = time.perf_counter()\n",
    "    actions = agent.register_reset(obs_dict)\n",
    "    agent_time_elapsed += time.perf_counter()- step_start\n",
    "\n",
    "    episodes_completed = 0\n",
    "    num_steps = 0\n",
    "    interrupted = False\n",
    "    episode_metrics = []\n",
    "    \n",
    "    # Skipping to shorten training time\n",
    "    days = 30*5\n",
    "    training_steps = 24*days\n",
    "    skipping = False\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            ### This is only a reference script provided to allow you \n",
    "            ### to do local evaluation. The evaluator **DOES NOT** \n",
    "            ### use this script for orchestrating the evaluations. \n",
    "\n",
    "            observations, _, done, _ = env.step(actions)\n",
    "            if done or skipping:\n",
    "                episodes_completed += 1\n",
    "                metrics_t = env.evaluate()\n",
    "                metrics = {\"price_cost\": metrics_t[0], \"emmision_cost\": metrics_t[1]}\n",
    "                if np.any(np.isnan(metrics_t)):\n",
    "                    raise ValueError(\"Episode metrics are nan, please contant organizers\")\n",
    "                episode_metrics.append(metrics)\n",
    "                print(f\"Episode complete: {episodes_completed} | Latest episode metrics: {metrics}\", )\n",
    "\n",
    "                obs_dict = env_reset(env)\n",
    "\n",
    "                step_start = time.perf_counter()\n",
    "                actions = agent.register_reset(obs_dict)\n",
    "                agent_time_elapsed += time.perf_counter()- step_start\n",
    "            else:\n",
    "                step_start = time.perf_counter()\n",
    "                actions = agent.compute_action(observations)\n",
    "                agent_time_elapsed += time.perf_counter()- step_start\n",
    "            \n",
    "            num_steps += 1\n",
    "            if num_steps % 1000 == 0:\n",
    "                print(f\"Num Steps: {num_steps}, Num episodes: {episodes_completed}\")\n",
    "            \n",
    "            ### End training in set time\n",
    "            if num_steps % training_steps == 0:\n",
    "                print(f\"Num Steps: {num_steps}, Num episodes: {episodes_completed}\")\n",
    "                if num_steps == training_steps:\n",
    "                    print(f'ENDING TRAINING AFTER {training_steps} STEPS')\n",
    "                    skipping = True\n",
    "\n",
    "            if episodes_completed >= Constants.episodes:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"========================= Stopping Evaluation =========================\")\n",
    "        interrupted = True\n",
    "    \n",
    "    if not interrupted:\n",
    "        print(\"=========================Completed=========================\")\n",
    "\n",
    "    if len(episode_metrics) > 0:\n",
    "        print(\"Average Price Cost:\", np.mean([e['price_cost'] for e in episode_metrics]))\n",
    "        print(\"Average Emmision Cost:\", np.mean([e['emmision_cost'] for e in episode_metrics]))\n",
    "    print(f\"Total time taken by agent: {agent_time_elapsed}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# setting Up Environment requiremnents.txt and yml files \n",
    "\n",
    "follow the links https://stackoverflow.com/questions/48787250/set-up-virtualenv-using-a-requirements-txt-generated-by-conda"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15c682508d24c5a67d5fbbe2fea073172b3a3ca9fabad277d65ba1c57669360a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('CityLearn2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}